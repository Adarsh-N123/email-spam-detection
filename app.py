def filter(sentence):
    # -*- coding: utf-8 -*-
  """spam_filtering.ipynb

  Automatically generated by Colaboratory.

  Original file is located at
      https://colab.research.google.com/drive/1aybFE8MZfdzX3NW7UxKxeILDUxKOMNQT
  """

  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns

  df = pd.read_csv("emails.csv", encoding="ISO-8859-1")

  df

  # sns.heatmap(df.isnull())

  # df = df.drop(["Unnamed: 2","Unnamed: 3","Unnamed: 4"],axis=1)

  df.head()

  from sklearn.preprocessing import LabelEncoder

  encoder = LabelEncoder()
  df["spam"]=encoder.fit_transform(df["spam"])
  #spam 1 and ham 0

  df.head()

  df.duplicated().sum()

  df = df.drop_duplicates(keep="first")

  df["spam"].value_counts()

  # !pip install nltk
  import nltk

  nltk.download('punkt')

  df["num_characters"]=df["text"].apply(len)

  df["word_count"]=df["text"].apply(lambda x:len(nltk.word_tokenize(x)))

  df["sent_count"]=df["text"].apply(lambda x:len(nltk.sent_tokenize(x)))

  df.head()

  df[df["spam"]==0][["num_characters","word_count","sent_count"]].describe()

  df[df["spam"]==1][["num_characters","word_count","sent_count"]].describe()

  # sns.pairplot(df,hue="v1")

  df.corr()

  # sns.heatmap(df.corr(),annot=True)

  from nltk.corpus import stopwords
  nltk.download('stopwords')
  stop_words1 = stopwords.words('english')

  import string
  string.punctuation

  def transform(text):
    text = text.lower()
    text = nltk.word_tokenize(text)
    y=[]
    for i in text:
      if i.isalnum():
        y.append(i)
    text = y[:]
    y.clear()
    for i in text:
      if i not in stop_words1 and i not in string.punctuation:
        y.append(i)
    from nltk.stem.porter import PorterStemmer
    ps = PorterStemmer()
    z=[]
    for i in y:
      z.append(ps.stem(i))

    return " ".join(z)

  transform("Hi @ my name is! adarsh and i am scammed in . ")

  df["transformed_text"]=df["text"].apply(transform)
  df.head()

  from wordcloud import WordCloud
  wc = WordCloud(width=500,height=500,min_font_size=10,background_color="grey")

  # spam_wc = wc.generate(df[df["spam"]==1]["transformed_text"].str.cat(sep=" "))
  # plt.imshow(spam_wc)

  # spam_wc = wc.generate(df[df["spam"]==0]["transformed_text"].str.cat(sep=" "))
  # plt.imshow(spam_wc)

  spam_corpus=[]
  for sen in df[df["spam"]==1]["transformed_text"].tolist():
    for word in sen.split():
      spam_corpus.append(word)
  from collections import Counter
  # sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])
  # plt.xticks(rotation="vertical")
  # plt.show()

  spam_corpus=[]
  for sen in df[df["spam"]==0]["transformed_text"].tolist():
    for word in sen.split():
      spam_corpus.append(word)
  from collections import Counter
  # sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])
  # plt.xticks(rotation="vertical")
  # plt.show()

  """**MODEL BUILDING**"""

  from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
  cv = TfidfVectorizer(max_features=3000)#CountVectorizer()



  x=cv.fit_transform(df["transformed_text"]).toarray()

  x.shape

  y = df["spam"].values

  y

  # from sklearn.model_selection import train_test_split
  # x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=101)

  from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB

  gnb = GaussianNB()
  mnb = MultinomialNB()
  bnb = BernoulliNB()
  from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

  # gnb.fit(x_train,y_train)
  # predict_gnb = gnb.predict(x_test)
  # print(accuracy_score(y_test,predict_gnb))
  # print(confusion_matrix(y_test,predict_gnb))
  # print(precision_score(y_test,predict_gnb))
  # x_test[0]

  mnb.fit(x,y)
  final1 = transform(sentence)
  print(final1)
  d = {'col1':[final1]}
  df1 = pd.DataFrame(data=d)
  # cv = TfidfVectorizer(max_features=3000)#CountVectorizer()
  # final2 = cv.fit_transform(df1["col1"]).toarray()
  # print(final2)
  # cv = TfidfVectorizer(max_features=3000)#CountVectorizer()
  final2=cv.transform(df1["col1"]).toarray()
  
  predict_gnb = mnb.predict(final2)
  if predict_gnb[0]==0:
    return "NOT A SPAM"
  else:
    return "SPAM!!!"
  # print(accuracy_score(y_test,predict_gnb))
  # # print(confusion_matrix(y_test,predict_gnb))
  # print(precision_score(y_test,predict_gnb))
  # # predict_gnb[0]

  # bnb.fit(x_train,y_train)
  # predict_gnb = bnb.predict(x_test)
  # print(accuracy_score(y_test,predict_gnb))
  # print(confusion_matrix(y_test,predict_gnb))
  # print(precision_score(y_test,predict_gnb))

  # import pickle
  # pickle.dump(cv, open("TfidfVectorizer.pkl", "wb"))
  # pickle.dump(mnb, open("MultinomialNB.pkl", "wb"))
import gradio as gr



outputs = gr.outputs.Textbox()

app = gr.Interface(fn=filter, inputs="text", outputs=outputs,description="This is a Spam filtering model")
app.launch()
